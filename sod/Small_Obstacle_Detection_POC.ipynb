{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATwcAEhAL5sv"
   },
   "source": [
    "# Problem description\n",
    "\n",
    "The majority of available models, trained for object detection and recognition tasks in the field of autonomous/automated driving systems, considers only large objects such as trees on the side of the road, pedestrians, surrounding vehicles, large animals or road blockages.\n",
    "\n",
    "Detecting small (low-level) obstacles on the road has posed a challenge, mainly due to the noise, or skew, in their pixel frequency or the small size of features that can describe these obstacles relative to the size of the frame. It is difficult for Neural Networks to approximate these type of objects, therefore many times they are randomly classified.\n",
    "\n",
    "Recently, however, small obstacle detection has gained more popularity as  the demand for fully automated vehicles rose. Detecting unexpected small obstacles on the road could prevent the accidents caused by falling debris, construction activities or lost cargo, etc, providing a safer driving experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSAqUw2sPXRc"
   },
   "source": [
    "# About the Dataset\n",
    "\n",
    "The **Lost and Found** dataset contains images combined with RGB depth information used to segment the image, determining the pixels that belong either to obstacles, road or non-road surfaces. \n",
    "\n",
    "The problem with this dataset is that it has a limited depth of 20m and the poor accuracy of detecting very small objects.\n",
    "\n",
    "The **Small Obstacle Dataset**, created by the Robotics Research Center IIIT from India, collected images as well as sensor data using a highly accurate Lidar sensor, detecting objects up to a depth of 75m. The data between the 2 devices is calibrated in order to obtain a better representation of the driving conditions. The images as well as the sensor data are labeled in order to detect only small level obstacles, which means they are specialized exactly for this type of task.\n",
    "\n",
    "It consists of 2 sets: one containing data obtained from real-life situations, while the second set contains data from a simulator in Unreal Engine.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing the necessary libraries. The images as well as the point clouds will be plotted using the plotly.matplotlib library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import open3d as o3d\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import keras\n",
    "import scipy\n",
    "import fnmatch\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ------------------real data-----------------------------\n",
    "TRAIN_BASE_DIR = \"datasets/small_obs_dataset/Small_Obstacle_Dataset/train\"\n",
    "VAL_BASE_DIR = \"datasets/small_obs_dataset/Small_Obstacle_Dataset/val\"\n",
    "TEST_BASE_DIR = \"datasets/small_obs_dataset/Small_Obstacle_Dataset/test\"\n",
    "\n",
    "LABELS_DIR = \"/labels\"\n",
    "IMAGE_DIR = \"/image\"\n",
    "ODOMETRY_DIR = \"/odometry\"\n",
    "VELODYNE_DIR = \"/velodyne\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualize the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def view_image_input(image_dir, labels_dir, input_file_name):\n",
    "    img_path = image_dir + \"/\" + input_file_name\n",
    "    img = mpimg.imread(img_path)\n",
    "\n",
    "    segm_path = labels_dir + \"/\" + input_file_name\n",
    "    segm = mpimg.imread(segm_path)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 15))\n",
    "\n",
    "    ax1.set_title(\"Input image\")\n",
    "    ax1.imshow(img)\n",
    "\n",
    "    ax2.set_title(\"Segmentation mask\")\n",
    "    ax2.imshow(segm)\n",
    "\n",
    "    plt.show(block=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Real image sample:\")\n",
    "view_image_input(TRAIN_BASE_DIR + \"/file_1\" + IMAGE_DIR, TRAIN_BASE_DIR + \"/file_1\" + LABELS_DIR, \"0000000080.png\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def save_to_csv(output_path, headers, data):\n",
    "    with open(output_path, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # write headers in first row\n",
    "        writer.writerow(headers)\n",
    "        # write the data from the given list\n",
    "        for row in data:\n",
    "            writer.writerow(row)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load image data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_image_data(root_dir):\n",
    "    data = []\n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        if root.endswith(\"image\"):\n",
    "            files = [os.path.join(root, f).replace(\"\\\\\", \"/\") for f in files if f.endswith('.png')]\n",
    "            images = list(filter(lambda f: os.path.isfile(f.replace(\"/image\", \"/labels\")), files))\n",
    "            labels = [img.replace(\"/image\", \"/labels\") for img in images]\n",
    "            data += map(lambda t: [t[0], t[1]], zip(images, labels))\n",
    "\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Store the path to the images and their respective labels, in a csv file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_img_csv_path = TRAIN_BASE_DIR + '/train_images.csv'\n",
    "val_img_csv_path = VAL_BASE_DIR + '/val_images.csv'\n",
    "test_img_csv_path = TEST_BASE_DIR + '/test_images.csv'\n",
    "\n",
    "img_label_headers = [\"image\", \"label\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_img_data = load_image_data(TRAIN_BASE_DIR)\n",
    "print(\"Found %d train images along with their semantic masks\" % len(train_img_data))\n",
    "# save to csv file\n",
    "save_to_csv(train_img_csv_path, img_label_headers, train_img_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "val_img_data = load_image_data(VAL_BASE_DIR)\n",
    "print(\"Found %d val images along with their semantic masks\" % len(val_img_data))\n",
    "# save to csv file\n",
    "save_to_csv(val_img_csv_path, img_label_headers, val_img_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_img_data = load_image_data(TEST_BASE_DIR)\n",
    "print(\"Found %d test images along with their semantic masks\" % len(test_img_data))\n",
    "# save to csv file\n",
    "save_to_csv(test_img_csv_path, img_label_headers, test_img_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "IMG_HEIGHT = 180\n",
    "IMG_WIDTH = 180\n",
    "BATCH_SIZE = 32"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_image_df = pd.read_csv(train_img_csv_path)\n",
    "print(\"Columns: \", train_image_df.columns.values)\n",
    "print(\"Shape: \", train_image_df.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_image_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(path):\n",
    "    img_file = tf.io.read_file(path)\n",
    "    img_array = tf.io.decode_png(img_file, channels=3)\n",
    "    img_resized = tf.image.resize(img_array, [IMG_HEIGHT, IMG_WIDTH])\n",
    "    return img_resized\n",
    "\n",
    "\n",
    "def load_and_preprocess_image_and_label(row):\n",
    "    img_data = load_and_preprocess_image(row[0])\n",
    "    label_data = load_and_preprocess_image(row[1])\n",
    "    return img_data, label_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_tensor = tf.data.Dataset.from_tensor_slices(train_image_df[['image', 'label']].values)\n",
    "train_tensor = train_tensor.map(load_and_preprocess_image_and_label, tf.data.experimental.AUTOTUNE)\n",
    "print(train_tensor.element_spec)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "it = train_tensor.shuffle(len(train_image_df.values)).batch(32).as_numpy_iterator()\n",
    "it.next()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lost and Found dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lost_found_dataset, info = tfds.load('lost_and_found',\n",
    "                                     with_info=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The lost and found dataset contains 2104 annotated frames gathered from 112 video seqeunces: http://wwwlehre.dhbw-stuttgart.de/~sgehrig/lostAndFoundDataset/index.html"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "info.features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lost_found_dataset['train']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "View a few sample images from the dataset:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_ = tfds.visualization.show_examples(lost_found_dataset['train'], info, image_key=\"image_left\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_ = tfds.visualization.show_examples(lost_found_dataset['train'], info, image_key=\"segmentation_label\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lost_found_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Restricted Boltzmann Machine"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RBM():\n",
    "    def __init__(self, nv, nh):\n",
    "        \"\"\"\n",
    "\n",
    "        :param nv: nr of neurons in the input/visible layer\n",
    "        :param nh: nr of neurons in the hidden layer\n",
    "        \"\"\"\n",
    "        # initialize the weight matrix\n",
    "        self.W = tf.Variable(tf.truncated_normal((nv, nh)) * 0.01)\n",
    "        self.bv = tf.Variable(tf.zeros((nv, 1)))\n",
    "        self.bh = tf.Variable(tf.zeros((nh, 1)))\n",
    "\n",
    "    def bernoulli(self, p):\n",
    "        return tf.nn.relu(tf.sign(p = tf.random_uniform(p.shape)))\n",
    "\n",
    "    def energy_function(self, v):\n",
    "        b = tf.matmul(v, self.bv)\n",
    "        linear_tr = tf.matmul(v, self.W) + tf.squeeze(self.bh)\n",
    "        h = tf.reduce_sum(tf.log(tf.exp(linear_tr) + 1), axis=1)\n",
    "        return tf.reduce_mean(-h, -b)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
