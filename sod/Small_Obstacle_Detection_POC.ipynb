{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATwcAEhAL5sv"
   },
   "source": [
    "# Problem description\n",
    "\n",
    "The majority of available models, trained for object detection and recognition tasks in the field of autonomous/automated driving systems, considers only large objects such as trees on the side of the road, pedestrians, surrounding vehicles, large animals or road blockages.\n",
    "\n",
    "Detecting small (low-level) obstacles on the road has posed a challenge, mainly due to the noise, or skew, in their pixel frequency or the small size of features that can describe these obstacles relative to the size of the frame. It is difficult for Neural Networks to approximate these type of objects, therefore many times they are randomly classified.\n",
    "\n",
    "Recently, however, small obstacle detection has gained more popularity as  the demand for fully automated vehicles rose. Detecting unexpected small obstacles on the road could prevent the accidents caused by falling debris, construction activities or lost cargo, etc, providing a safer driving experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSAqUw2sPXRc"
   },
   "source": [
    "# About the Dataset\n",
    "\n",
    "The **Lost and Found** dataset contains images combined with RGB depth information used to segment the image, determining the pixels that belong either to obstacles, road or non-road surfaces. \n",
    "\n",
    "The problem with this dataset is that it has a limited depth of 20m and the poor accuracy of detecting very small objects.\n",
    "\n",
    "The **Small Obstacle Dataset**, created by the Robotics Research Center IIIT from India, collected images as well as sensor data using a highly accurate Lidar sensor, detecting objects up to a depth of 75m. The data between the 2 devices is calibrated in order to obtain a better representation of the driving conditions. The images as well as the sensor data are labeled in order to detect only small level obstacles, which means they are specialized exactly for this type of task.\n",
    "\n",
    "It consists of 2 sets: one containing data obtained from real-life situations, while the second set contains data from a simulator in Unreal Engine.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing the necessary libraries. The images as well as the point clouds will be plotted using the plotly.matplotlib library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import open3d as o3d\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import keras\n",
    "import scipy\n",
    "import fnmatch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------real data-----------------------------\n",
    "TRAIN_BASE_DIR = \"datasets/small_obs_dataset/Small_Obstacle_Dataset/train\"\n",
    "VAL_BASE_DIR = \"datasets/small_obs_dataset/Small_Obstacle_Dataset/val\"\n",
    "TEST_BASE_DIR = \"datasets/small_obs_dataset/Small_Obstacle_Dataset/test\"\n",
    "\n",
    "LABELS_DIR = \"/labels\"\n",
    "IMAGE_DIR = \"/image\"\n",
    "ODOMETRY_DIR = \"/odometry\"\n",
    "VELODYNE_DIR = \"/velodyne\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def save_to_csv(output_path, headers, data):\n",
    "    with open(output_path, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # write headers in first row\n",
    "        writer.writerow(headers)\n",
    "        # write the data from the given list\n",
    "        for row in data:\n",
    "            writer.writerow(row)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load image data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def load_image_data(root_dir):\n",
    "    data = []\n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        if root.endswith(\"image\"):\n",
    "            files = [os.path.join(root, f).replace(\"\\\\\", \"/\") for f in files if f.endswith('.png')]\n",
    "            images = list(filter(lambda f: os.path.isfile(f.replace(\"/image\", \"/labels\")), files))\n",
    "            labels = [img.replace(\"/image\", \"/labels\") for img in images]\n",
    "            data += map(lambda t: [t[0], t[1]], zip(images, labels))\n",
    "\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Store the path to the images and their respective labels, in a csv file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "train_img_csv_path = TRAIN_BASE_DIR + '/train_images.csv'\n",
    "val_img_csv_path = VAL_BASE_DIR + '/val_images.csv'\n",
    "test_img_csv_path = TEST_BASE_DIR + '/test_images.csv'\n",
    "\n",
    "img_label_headers = [\"image\", \"label\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1932 train images along with their semantic masks\n"
     ]
    }
   ],
   "source": [
    "train_img_data = load_image_data(TRAIN_BASE_DIR)\n",
    "print(\"Found %d train images along with their semantic masks\" % len(train_img_data))\n",
    "# save to csv file\n",
    "save_to_csv(train_img_csv_path, img_label_headers, train_img_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 496 val images along with their semantic masks\n"
     ]
    }
   ],
   "source": [
    "val_img_data = load_image_data(VAL_BASE_DIR)\n",
    "print(\"Found %d val images along with their semantic masks\" % len(val_img_data))\n",
    "# save to csv file\n",
    "save_to_csv(val_img_csv_path, img_label_headers, val_img_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 491 test images along with their semantic masks\n"
     ]
    }
   ],
   "source": [
    "test_img_data = load_image_data(TEST_BASE_DIR)\n",
    "print(\"Found %d test images along with their semantic masks\" % len(test_img_data))\n",
    "# save to csv file\n",
    "save_to_csv(test_img_csv_path, img_label_headers, test_img_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "IMG_HEIGHT = 180\n",
    "IMG_WIDTH = 180\n",
    "BATCH_SIZE = 32"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:  ['image' 'label']\n",
      "Shape:  (1932, 2)\n"
     ]
    }
   ],
   "source": [
    "train_image_df = pd.read_csv(train_img_csv_path)\n",
    "print(\"Columns: \", train_image_df.columns.values)\n",
    "print(\"Shape: \", train_image_df.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "                                               image  \\\n0  datasets/small_obs_dataset/Small_Obstacle_Data...   \n1  datasets/small_obs_dataset/Small_Obstacle_Data...   \n2  datasets/small_obs_dataset/Small_Obstacle_Data...   \n3  datasets/small_obs_dataset/Small_Obstacle_Data...   \n4  datasets/small_obs_dataset/Small_Obstacle_Data...   \n\n                                               label  \n0  datasets/small_obs_dataset/Small_Obstacle_Data...  \n1  datasets/small_obs_dataset/Small_Obstacle_Data...  \n2  datasets/small_obs_dataset/Small_Obstacle_Data...  \n3  datasets/small_obs_dataset/Small_Obstacle_Data...  \n4  datasets/small_obs_dataset/Small_Obstacle_Data...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>datasets/small_obs_dataset/Small_Obstacle_Data...</td>\n      <td>datasets/small_obs_dataset/Small_Obstacle_Data...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>datasets/small_obs_dataset/Small_Obstacle_Data...</td>\n      <td>datasets/small_obs_dataset/Small_Obstacle_Data...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>datasets/small_obs_dataset/Small_Obstacle_Data...</td>\n      <td>datasets/small_obs_dataset/Small_Obstacle_Data...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>datasets/small_obs_dataset/Small_Obstacle_Data...</td>\n      <td>datasets/small_obs_dataset/Small_Obstacle_Data...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>datasets/small_obs_dataset/Small_Obstacle_Data...</td>\n      <td>datasets/small_obs_dataset/Small_Obstacle_Data...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_image_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(path):\n",
    "    img_file = tf.io.read_file(path)\n",
    "    img_array = tf.io.decode_png(img_file, channels=3)\n",
    "    img_resized = tf.image.resize(img_array, [IMG_HEIGHT, IMG_WIDTH])\n",
    "    return img_resized\n",
    "\n",
    "\n",
    "def load_and_preprocess_image_and_label(row):\n",
    "    img_data = load_and_preprocess_image(row[0])\n",
    "    label_data = load_and_preprocess_image(row[1])\n",
    "    return img_data, label_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(180, 180, 3), dtype=tf.float32, name=None), TensorSpec(shape=(180, 180, 3), dtype=tf.float32, name=None))\n"
     ]
    }
   ],
   "source": [
    "train_tensor = tf.data.Dataset.from_tensor_slices(train_image_df[['image', 'label']].values)\n",
    "train_tensor = train_tensor.map(load_and_preprocess_image_and_label, tf.data.experimental.AUTOTUNE)\n",
    "print(train_tensor.element_spec)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([[[[ 10.527778 ,  14.527778 ,  13.527778 ],\n          [ 10.       ,  14.       ,  13.       ],\n          [  9.5      ,  13.5      ,  12.5      ],\n          ...,\n          [ 11.55481  ,  24.138245 ,  23.2771   ],\n          [158.8324   , 158.99902  , 168.3324   ],\n          [ 28.138855 ,  30.111084 ,  37.638855 ]],\n \n         [[ 12.527778 ,  16.527779 ,  15.527778 ],\n          [ 13.416666 ,  17.416666 ,  16.416666 ],\n          [  7.5      ,  11.5      ,  10.5      ],\n          ...,\n          [ 26.44458  ,  32.083435 ,  33.94458  ],\n          [ 50.916443 ,  55.916443 ,  59.916443 ],\n          [ 24.333374 ,  29.861145 ,  33.333374 ]],\n \n         [[  7.9444447,  11.944445 ,  10.944445 ],\n          [  7.5      ,  11.5      ,  10.5      ],\n          [  6.1388893,   9.       ,   9.       ],\n          ...,\n          [ 25.305725 ,  31.305725 ,  34.805725 ],\n          [ 19.749939 ,  25.749939 ,  27.833252 ],\n          [ 20.527771 ,  26.527771 ,  28.527771 ]],\n \n         ...,\n \n         [[ 81.47222  ,  83.44444  ,  75.94444  ],\n          [ 98.83333  , 102.25     ,  88.66667  ],\n          [ 93.75     ,  98.111115 ,  85.52779  ],\n          ...,\n          [ 32.       ,  38.       ,  50.       ],\n          [ 44.41693  ,  50.91693  ,  63.41693  ],\n          [ 75.05554  ,  81.05554  ,  95.02777  ]],\n \n         [[ 87.5      ,  88.97222  ,  80.       ],\n          [ 96.25     ,  98.75     ,  85.833336 ],\n          [ 95.91667  ,  97.55556  ,  89.833336 ],\n          ...,\n          [ 30.861145 ,  36.861145 ,  48.861145 ],\n          [ 38.833496 ,  44.833496 ,  56.833496 ],\n          [ 74.388916 ,  81.361145 ,  92.888916 ]],\n \n         [[ 93.52778  ,  96.02778  ,  91.02778  ],\n          [ 89.58333  ,  92.58333  ,  87.58333  ],\n          [ 90.638885 ,  93.638885 ,  89.138885 ],\n          ...,\n          [ 27.638855 ,  33.638855 ,  45.5      ],\n          [ 33.250183 ,  39.250183 ,  51.250183 ],\n          [ 70.27783  ,  76.27783  ,  88.27783  ]]],\n \n \n        [[[102.638885 , 136.61111  ,  77.138885 ],\n          [102.91667  , 132.41666  ,  89.5      ],\n          [ 53.111107 ,  76.69444  ,  44.027775 ],\n          ...,\n          [ 37.94397  ,  54.305115 ,  32.305115 ],\n          [ 42.333252 ,  55.08319  ,  35.333252 ],\n          [ 18.472229 ,  28.472229 ,  14.444458 ]],\n \n         [[ 70.05556  , 108.611115 ,  45.5      ],\n          [155.5      , 196.41666  , 139.33333  ],\n          [185.83334  , 220.97223  , 180.47223  ],\n          ...,\n          [ 42.44458  ,  57.44458  ,  38.44458  ],\n          [ 52.99878  ,  64.33203  ,  48.99878  ],\n          [ 16.249939 ,  26.249939 ,  14.222168 ]],\n \n         [[144.41667  , 187.38889  , 132.86111  ],\n          [ 75.916664 , 121.41667  ,  62.416668 ],\n          [109.97223  , 148.97223  ,  99.47223  ],\n          ...,\n          [ 33.55542  ,  47.05542  ,  30.27771  ],\n          [ 20.5      ,  30.583313 ,  17.416687 ],\n          [ 17.       ,  25.       ,  13.972229 ]],\n \n         ...,\n \n         [[ 17.944445 ,  23.944445 ,  27.944445 ],\n          [ 15.916666 ,  18.916666 ,  23.916666 ],\n          [ 14.5      ,  17.5      ,  22.5      ],\n          ...,\n          [ 40.83313  ,  46.194275 ,  53.83313  ],\n          [ 34.083313 ,  39.083313 ,  47.083313 ],\n          [ 33.5      ,  38.5      ,  46.5      ]],\n \n         [[ 19.       ,  24.88889  ,  28.944445 ],\n          [ 17.       ,  20.       ,  25.       ],\n          [ 15.861111 ,  18.86111  ,  23.86111  ],\n          ...,\n          [ 68.915955 ,  73.915955 ,  80.915955 ],\n          [ 36.166504 ,  41.166504 ,  49.166504 ],\n          [ 32.97223  ,  37.97223  ,  45.97223  ]],\n \n         [[ 19.527779 ,  23.944445 ,  28.944445 ],\n          [ 17.5      ,  20.5      ,  25.5      ],\n          [ 16.5      ,  19.5      ,  24.5      ],\n          ...,\n          [ 77.38916  ,  83.028015 ,  90.528015 ],\n          [ 81.49951  ,  86.49951  ,  94.49951  ],\n          [ 34.888916 ,  39.888916 ,  47.888916 ]]],\n \n \n        [[[ 55.52778  ,  81.02778  ,  64.02778  ],\n          [ 57.333332 ,  82.666664 ,  66.25     ],\n          [ 37.305553 ,  64.94444  ,  46.444443 ],\n          ...,\n          [ 25.44458  ,  28.861145 ,  33.138855 ],\n          [ 38.       ,  41.       ,  44.       ],\n          [ 23.555542 ,  27.055542 ,  28.055542 ]],\n \n         [[ 71.416664 , 103.416664 ,  81.388885 ],\n          [ 86.16667  , 117.16667  ,  93.58333  ],\n          [103.8889   , 134.3889   , 109.33334  ],\n          ...,\n          [ 46.916565 ,  48.55542  ,  55.55542  ],\n          [ 21.583313 ,  25.666626 ,  30.583313 ],\n          [ 19.5      ,  23.       ,  23.555542 ]],\n \n         [[ 54.333336 ,  83.52778  ,  63.333336 ],\n          [127.33333  , 167.33333  , 132.       ],\n          [107.0278   , 151.6667   , 116.666695 ],\n          ...,\n          [ 33.44275  ,  29.442749 ,  46.44275  ],\n          [ 19.583313 ,  25.       ,  30.166626 ],\n          [ 18.       ,  23.5      ,  23.055542 ]],\n \n         ...,\n \n         [[161.       , 164.       , 161.       ],\n          [158.66667  , 162.16667  , 159.16667  ],\n          [177.       , 181.       , 178.       ],\n          ...,\n          [ 68.77771  ,  80.27771  , 104.27771  ],\n          [109.08356  , 120.000244 , 141.66687  ],\n          [113.138855 , 123.138855 , 144.11108  ]],\n \n         [[161.47223  , 165.47223  , 162.47223  ],\n          [161.5      , 165.5      , 162.5      ],\n          [168.86111  , 172.86111  , 169.86111  ],\n          ...,\n          [ 66.       ,  78.       , 102.       ],\n          [ 98.16687  , 110.16687  , 134.16687  ],\n          [117.94446  , 128.       , 150.       ]],\n \n         [[159.02777  , 163.       , 160.       ],\n          [163.91667  , 167.91667  , 164.91667  ],\n          [166.16666  , 172.16666  , 168.66666  ],\n          ...,\n          [ 61.77771  ,  73.638855 ,  97.638855 ],\n          [ 85.500244 ,  98.91693  , 122.41693  ],\n          [111.97223  , 124.94446  , 146.97223  ]]],\n \n \n        ...,\n \n \n        [[[200.97223  , 192.97223  , 236.97223  ],\n          [218.75     , 207.75     , 235.58333  ],\n          [240.25     , 225.97223  , 246.69444  ],\n          ...,\n          [253.86115  , 253.       , 254.22229  ],\n          [254.       , 253.       , 254.58331  ],\n          [183.86078  , 158.91632  , 208.30524  ]],\n \n         [[227.72223  , 220.72223  , 247.22223  ],\n          [253.41666  , 250.5      , 254.5      ],\n          [252.5      , 253.       , 255.       ],\n          ...,\n          [247.38916  , 246.19489  , 247.7503   ],\n          [254.5      , 251.66663  , 254.5      ],\n          [ 38.277466 ,  23.499878 ,  88.416565 ]],\n \n         [[194.41666  , 206.69444  , 229.13889  ],\n          [186.91667  , 172.       , 204.83334  ],\n          [213.5278   , 207.22224  , 230.97223  ],\n          ...,\n          [169.97443  , 163.83557  , 175.47443  ],\n          [254.       , 244.91644  , 254.       ],\n          [ 25.416565 ,  29.528015 ,  85.777954 ]],\n \n         ...,\n \n         [[169.08334  , 173.05556  , 170.05556  ],\n          [173.58334  , 177.91666  , 173.08334  ],\n          [176.30554  , 180.16666  , 176.80554  ],\n          ...,\n          [ 51.416565 ,  61.916565 ,  79.77771  ],\n          [ 73.75043  ,  88.417175 , 106.0838   ],\n          [119.138855 , 129.13885  , 150.11108  ]],\n \n         [[171.94444  , 175.44444  , 171.44444  ],\n          [171.41666  , 175.08334  , 171.       ],\n          [164.5      , 168.5      , 165.5      ],\n          ...,\n          [ 52.638855 ,  62.138855 ,  80.27771  ],\n          [ 66.750305 ,  76.750305 ,  97.750305 ],\n          [120.44446  , 132.5      , 152.97223  ]],\n \n         [[175.33333  , 180.36111  , 172.33333  ],\n          [170.58334  , 175.58334  , 168.16666  ],\n          [167.22223  , 172.22223  , 166.86111  ],\n          ...,\n          [ 49.05542  ,  59.27771  ,  78.       ],\n          [ 58.250305 ,  70.250305 ,  90.66699  ],\n          [120.861145 , 130.86115  , 152.38892  ]]],\n \n \n        [[[  8.       ,  12.       ,   9.       ],\n          [  9.416666 ,  13.416666 ,  10.416666 ],\n          [  8.638889 ,  12.5      ,   9.5      ],\n          ...,\n          [246.91656  , 235.27771  , 247.27771  ],\n          [245.75006  , 237.75006  , 246.83337  ],\n          [247.63885  , 244.11108  , 249.61108  ]],\n \n         [[  8.5      ,  12.5      ,   9.5      ],\n          [ 10.583334 ,  14.583334 ,  11.583334 ],\n          [ 11.5      ,  15.5      ,  12.5      ],\n          ...,\n          [245.41656  , 235.41656  , 247.41656  ],\n          [246.66675  , 239.25006  , 247.75006  ],\n          [251.52777  , 244.16663  , 252.97223  ]],\n \n         [[ 12.972222 ,  16.972221 ,  13.972222 ],\n          [ 12.416666 ,  16.416666 ,  13.416666 ],\n          [ 13.       ,  17.       ,  14.       ],\n          ...,\n          [247.41656  , 239.27771  , 247.27771  ],\n          [247.75006  , 244.25006  , 250.25006  ],\n          [249.63885  , 247.58331  , 255.       ]],\n \n         ...,\n \n         [[ 59.       ,  62.       ,  59.       ],\n          [ 59.       ,  62.       ,  59.       ],\n          [ 60.86111  ,  63.86111  ,  60.86111  ],\n          ...,\n          [ 18.138855 ,  20.5      ,  29.       ],\n          [ 22.083435 ,  27.083435 ,  35.083435 ],\n          [ 41.944458 ,  44.52777  ,  53.5      ]],\n \n         [[ 59.47222  ,  62.47222  ,  59.47222  ],\n          [ 59.166668 ,  62.166668 ,  59.166668 ],\n          [ 60.       ,  63.       ,  60.       ],\n          ...,\n          [ 15.       ,  18.       ,  25.       ],\n          [ 17.333374 ,  22.333374 ,  30.333374 ],\n          [ 38.888916 ,  43.888916 ,  51.888916 ]],\n \n         [[ 59.       ,  62.       ,  59.       ],\n          [ 58.416664 ,  61.416664 ,  58.416664 ],\n          [ 58.       ,  61.       ,  58.       ],\n          ...,\n          [ 13.638855 ,  16.5      ,  23.638855 ],\n          [ 15.416687 ,  17.916687 ,  26.5      ],\n          [ 33.333374 ,  38.333374 ,  46.333374 ]]],\n \n \n        [[[ 15.972222 ,  18.972221 ,  15.972222 ],\n          [ 15.5      ,  18.5      ,  15.5      ],\n          [ 16.       ,  19.       ,  16.       ],\n          ...,\n          [ 60.305725 ,  86.805725 ,  46.083435 ],\n          [ 61.666626 ,  87.666626 ,  48.083313 ],\n          [ 57.611084 ,  81.138855 ,  45.583313 ]],\n \n         [[ 16.027779 ,  19.027779 ,  16.027779 ],\n          [ 17.       ,  20.       ,  17.       ],\n          [ 16.       ,  19.       ,  16.       ],\n          ...,\n          [ 62.194275 ,  88.194275 ,  49.194275 ],\n          [ 51.583313 ,  75.       ,  41.       ],\n          [ 60.75006  ,  83.75006  ,  53.25006  ]],\n \n         [[ 17.472221 ,  20.472221 ,  17.472221 ],\n          [ 15.583334 ,  18.583334 ,  15.583334 ],\n          [ 17.5      ,  18.5      ,  16.       ],\n          ...,\n          [ 45.638855 ,  70.       ,  33.5      ],\n          [ 48.583313 ,  69.58331  ,  39.583313 ],\n          [ 47.055542 ,  69.55554  ,  40.       ]],\n \n         ...,\n \n         [[ 12.5      ,  17.5      ,  25.       ],\n          [ 58.08334  ,  64.08334  ,  65.08334  ],\n          [136.5      , 142.       , 137.08334  ],\n          ...,\n          [ 26.       ,  28.       ,  29.       ],\n          [ 29.000122 ,  31.000122 ,  32.083435 ],\n          [ 38.       ,  40.       ,  41.       ]],\n \n         [[ 19.305555 ,  22.305555 ,  26.305557 ],\n          [102.833336 , 106.833336 , 106.75     ],\n          [100.61111  , 103.61111  , 100.97221  ],\n          ...,\n          [ 24.5      ,  26.5      ,  27.5      ],\n          [ 23.916687 ,  25.916687 ,  28.916687 ],\n          [ 36.444458 ,  38.444458 ,  41.444458 ]],\n \n         [[ 19.11111  ,  22.11111  ,  25.63889  ],\n          [ 58.583332 ,  62.583332 ,  62.583332 ],\n          [ 17.027775 ,  19.027775 ,  19.027775 ],\n          ...,\n          [ 21.27771  ,  23.27771  ,  24.27771  ],\n          [ 21.416687 ,  23.416687 ,  26.333374 ],\n          [ 31.444458 ,  33.444458 ,  36.444458 ]]]], dtype=float32),\n array([[[[  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          ...,\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.]],\n \n         [[  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          ...,\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.]],\n \n         [[  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          ...,\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.]],\n \n         ...,\n \n         [[128.,   0.,   0.],\n          [128.,   0.,   0.],\n          [128.,   0.,   0.],\n          ...,\n          [128.,   0.,   0.],\n          [128.,   0.,   0.],\n          [128.,   0.,   0.]],\n \n         [[128.,   0.,   0.],\n          [128.,   0.,   0.],\n          [128.,   0.,   0.],\n          ...,\n          [128.,   0.,   0.],\n          [128.,   0.,   0.],\n          [128.,   0.,   0.]],\n \n         [[128.,   0.,   0.],\n          [128.,   0.,   0.],\n          [128.,   0.,   0.],\n          ...,\n          [128.,   0.,   0.],\n          [128.,   0.,   0.],\n          [128.,   0.,   0.]]],\n \n \n        [[[  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          ...,\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.]],\n \n         [[  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          ...,\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.]],\n \n         [[  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          ...,\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.]],\n \n         ...,\n \n         [[128.,   0.,   0.],\n          [128.,   0.,   0.],\n          [128.,   0.,   0.],\n          ...,\n          [128.,   0.,   0.],\n          [128.,   0.,   0.],\n          [128.,   0.,   0.]],\n \n         [[128.,   0.,   0.],\n          [128.,   0.,   0.],\n          [128.,   0.,   0.],\n          ...,\n          [128.,   0.,   0.],\n          [128.,   0.,   0.],\n          [128.,   0.,   0.]],\n \n         [[128.,   0.,   0.],\n          [128.,   0.,   0.],\n          [128.,   0.,   0.],\n          ...,\n          [128.,   0.,   0.],\n          [128.,   0.,   0.],\n          [128.,   0.,   0.]]],\n \n \n        [[[  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          ...,\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.]],\n \n         [[  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          ...,\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.]],\n \n         [[  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          ...,\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.]],\n \n         ...,\n \n         [[128.,   0.,   0.],\n          [128.,   0.,   0.],\n          [128.,   0.,   0.],\n          ...,\n          [128.,   0.,   0.],\n          [128.,   0.,   0.],\n          [128.,   0.,   0.]],\n \n         [[128.,   0.,   0.],\n          [128.,   0.,   0.],\n          [128.,   0.,   0.],\n          ...,\n          [128.,   0.,   0.],\n          [128.,   0.,   0.],\n          [128.,   0.,   0.]],\n \n         [[128.,   0.,   0.],\n          [128.,   0.,   0.],\n          [128.,   0.,   0.],\n          ...,\n          [128.,   0.,   0.],\n          [128.,   0.,   0.],\n          [128.,   0.,   0.]]],\n \n \n        ...,\n \n \n        [[[  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          ...,\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.]],\n \n         [[  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          ...,\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.]],\n \n         [[  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          ...,\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.]],\n \n         ...,\n \n         [[128.,   0.,   0.],\n          [128.,   0.,   0.],\n          [128.,   0.,   0.],\n          ...,\n          [128.,   0.,   0.],\n          [128.,   0.,   0.],\n          [128.,   0.,   0.]],\n \n         [[128.,   0.,   0.],\n          [128.,   0.,   0.],\n          [128.,   0.,   0.],\n          ...,\n          [128.,   0.,   0.],\n          [128.,   0.,   0.],\n          [128.,   0.,   0.]],\n \n         [[128.,   0.,   0.],\n          [128.,   0.,   0.],\n          [128.,   0.,   0.],\n          ...,\n          [128.,   0.,   0.],\n          [128.,   0.,   0.],\n          [128.,   0.,   0.]]],\n \n \n        [[[  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          ...,\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.]],\n \n         [[  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          ...,\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.]],\n \n         [[  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          ...,\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.]],\n \n         ...,\n \n         [[128.,   0.,   0.],\n          [128.,   0.,   0.],\n          [128.,   0.,   0.],\n          ...,\n          [128.,   0.,   0.],\n          [128.,   0.,   0.],\n          [128.,   0.,   0.]],\n \n         [[128.,   0.,   0.],\n          [128.,   0.,   0.],\n          [128.,   0.,   0.],\n          ...,\n          [128.,   0.,   0.],\n          [128.,   0.,   0.],\n          [128.,   0.,   0.]],\n \n         [[128.,   0.,   0.],\n          [128.,   0.,   0.],\n          [128.,   0.,   0.],\n          ...,\n          [128.,   0.,   0.],\n          [128.,   0.,   0.],\n          [128.,   0.,   0.]]],\n \n \n        [[[  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          ...,\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.]],\n \n         [[  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          ...,\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.]],\n \n         [[  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          ...,\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.]],\n \n         ...,\n \n         [[  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          ...,\n          [128.,   0.,   0.],\n          [128.,   0.,   0.],\n          [128.,   0.,   0.]],\n \n         [[  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          ...,\n          [128.,   0.,   0.],\n          [128.,   0.,   0.],\n          [128.,   0.,   0.]],\n \n         [[  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          [  0.,   0.,   0.],\n          ...,\n          [128.,   0.,   0.],\n          [128.,   0.,   0.],\n          [128.,   0.,   0.]]]], dtype=float32))"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it = train_tensor.shuffle(len(train_image_df.values)).batch(32).as_numpy_iterator()\n",
    "it.next()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load lidar data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def load_lidar_data(root_dir):\n",
    "    lidar_data = []\n",
    "    calibration_data = []\n",
    "    i = 0\n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        if root.endswith(\"velodyne\"):\n",
    "            files = [os.path.join(root, f).replace(\"\\\\\", \"/\") for f in files if f.endswith('.npy')]\n",
    "            lidar_data += list(map(lambda f: [f, i], files))\n",
    "            i += 1\n",
    "        calibration_data += [os.path.join(root, f).replace(\"\\\\\", \"/\") for f in files if f.endswith('.txt')]\n",
    "\n",
    "    data = list(map(lambda e: [e[0], calibration_data[e[1]]], lidar_data))\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Store the path to the lidar point cloud files in a csv file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "train_lidar_csv_path = TRAIN_BASE_DIR + '/train_lidar.csv'\n",
    "val_lidar_csv_path = VAL_BASE_DIR + '/val_lidar.csv'\n",
    "test_lidar_csv_path = TEST_BASE_DIR + '/test_lidar.csv'\n",
    "\n",
    "lidar_headers = [\"lidar\", \"calibration\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5696 train lidar npy files\n"
     ]
    }
   ],
   "source": [
    "train_lidar_data = load_lidar_data(TRAIN_BASE_DIR)\n",
    "print(\"Found %d train lidar npy files\" % len(train_lidar_data))\n",
    "# save to csv file\n",
    "save_to_csv(train_lidar_csv_path, lidar_headers, train_lidar_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1688 validation lidar npy files\n"
     ]
    }
   ],
   "source": [
    "val_lidar_data = load_lidar_data(VAL_BASE_DIR)\n",
    "print(\"Found %d validation lidar npy files\" % len(val_lidar_data))\n",
    "# save to csv file\n",
    "save_to_csv(val_lidar_csv_path, lidar_headers, val_lidar_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1130 test lidar npy files\n"
     ]
    }
   ],
   "source": [
    "test_lidar_data = load_lidar_data(TEST_BASE_DIR)\n",
    "print(\"Found %d test lidar npy files\" % len(test_lidar_data))\n",
    "# save to csv file\n",
    "save_to_csv(test_lidar_csv_path, lidar_headers, test_lidar_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:  ['lidar' 'calibration']\n",
      "Shape:  (5696, 2)\n"
     ]
    }
   ],
   "source": [
    "train_lidar_df = pd.read_csv(train_lidar_csv_path)\n",
    "print(\"Columns: \", train_lidar_df.columns.values)\n",
    "print(\"Shape: \", train_lidar_df.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "                                               lidar  \\\n0  datasets/small_obs_dataset/Small_Obstacle_Data...   \n1  datasets/small_obs_dataset/Small_Obstacle_Data...   \n2  datasets/small_obs_dataset/Small_Obstacle_Data...   \n3  datasets/small_obs_dataset/Small_Obstacle_Data...   \n4  datasets/small_obs_dataset/Small_Obstacle_Data...   \n\n                                         calibration  \n0  datasets/small_obs_dataset/Small_Obstacle_Data...  \n1  datasets/small_obs_dataset/Small_Obstacle_Data...  \n2  datasets/small_obs_dataset/Small_Obstacle_Data...  \n3  datasets/small_obs_dataset/Small_Obstacle_Data...  \n4  datasets/small_obs_dataset/Small_Obstacle_Data...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lidar</th>\n      <th>calibration</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>datasets/small_obs_dataset/Small_Obstacle_Data...</td>\n      <td>datasets/small_obs_dataset/Small_Obstacle_Data...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>datasets/small_obs_dataset/Small_Obstacle_Data...</td>\n      <td>datasets/small_obs_dataset/Small_Obstacle_Data...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>datasets/small_obs_dataset/Small_Obstacle_Data...</td>\n      <td>datasets/small_obs_dataset/Small_Obstacle_Data...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>datasets/small_obs_dataset/Small_Obstacle_Data...</td>\n      <td>datasets/small_obs_dataset/Small_Obstacle_Data...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>datasets/small_obs_dataset/Small_Obstacle_Data...</td>\n      <td>datasets/small_obs_dataset/Small_Obstacle_Data...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lidar_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def read_calibration_data(calibration_file):\n",
    "    with open(calibration_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        coord = []\n",
    "        for line in reader:\n",
    "            if len(line) > 0 and len(line[0]) > 0 and line[0][0] != '#':\n",
    "                # add each line which contains valid numbers (not comments '#' or empty strings ' '\n",
    "                # remove the last token, which is the empty space at the end of each line\n",
    "                coord.append(line[0].split(' ')[:-1])\n",
    "        # convert the string values to float\n",
    "        coord = np.asarray(coord, 'float64')\n",
    "        # read 7x4 values, from which:\n",
    "        ## first 3x4 values represent the intrinsic camera transformation matrix\n",
    "        ## the next 4x4 values represent the extrinsic lidar-camera transformation matrix\n",
    "\n",
    "        coord = np.reshape(coord, (7, 4))\n",
    "        int_mat = coord[:3]\n",
    "        ext_mat = coord[3:]\n",
    "\n",
    "        return int_mat, ext_mat\n",
    "\n",
    "def apply_camera_projection(points, camera_int, camera_ext):\n",
    "    project = lambda p, camera_int, camera_ext: camera_int @ camera_ext.dot(p[:4])\n",
    "    normalize = lambda p: (p / p[2])[:-1]\n",
    "    points[:,3] = 1\n",
    "    return [normalize(project(p, camera_int, camera_ext)) for p in points]\n",
    "\n",
    "def project_points_to_image(points, img_height, img_width):\n",
    "    # create a white canvas on which the points wil be projected\n",
    "    img_mat = np.full((img_height, img_width, 3), [255,255,255])\n",
    "    for p in points:\n",
    "        i = int(p[0]) % IMG_WIDTH\n",
    "        j = int(p[1]) % IMG_HEIGHT\n",
    "        img_mat[i][j] = [0,0,0]\n",
    "    return img_mat\n",
    "\n",
    "def load_and_preprocess_lidar(row):\n",
    "    lidar_path = row[0]\n",
    "    calibration_path = row[1]\n",
    "    pcd = np.load(lidar_path)\n",
    "    camera_int, camera_ext = read_calibration_data(calibration_path)\n",
    "    camera_int[0][2] = IMG_WIDTH/2\n",
    "    camera_int[1][2] = IMG_HEIGHT/2\n",
    "    camera_int[0][0] = 1/IMG_WIDTH\n",
    "    camera_int[1][1] = 1/IMG_HEIGHT\n",
    "    plane_projection = apply_camera_projection(pcd, camera_int, camera_ext)\n",
    "    img = project_points_to_image(plane_projection, 180, 180)\n",
    "    return img"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "train_lidar_proj_df = train_lidar_df[:BATCH_SIZE].apply(load_and_preprocess_lidar, axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorSpec(shape=(180, 180, 3), dtype=tf.int32, name=None)\n"
     ]
    }
   ],
   "source": [
    "train_lidar_tensor = tf.data.Dataset.from_tensor_slices(train_lidar_proj_df.tolist())\n",
    "print(train_lidar_tensor.element_spec)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x274e621b220>"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "it = train_lidar_tensor.shuffle(len(train_lidar_proj_df.values)).batch(32).as_numpy_iterator()\n",
    "sample_img = it.next()[10]\n",
    "plt.imshow(sample_img)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[1;32mD:\\university\\computer_vision\\python_interpreter\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py:103\u001B[0m, in \u001B[0;36mnormalize_element\u001B[1;34m(element, element_signature)\u001B[0m\n\u001B[0;32m    102\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m spec \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 103\u001B[0m     spec \u001B[38;5;241m=\u001B[39m \u001B[43mtype_spec_from_value\u001B[49m\u001B[43m(\u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_fallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    104\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m    105\u001B[0m   \u001B[38;5;66;03m# TypeError indicates it was not possible to compute a `TypeSpec` for\u001B[39;00m\n\u001B[0;32m    106\u001B[0m   \u001B[38;5;66;03m# the value. As a fallback try converting the value to a tensor.\u001B[39;00m\n",
      "File \u001B[1;32mD:\\university\\computer_vision\\python_interpreter\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py:487\u001B[0m, in \u001B[0;36mtype_spec_from_value\u001B[1;34m(element, use_fallback)\u001B[0m\n\u001B[0;32m    484\u001B[0m     logging\u001B[38;5;241m.\u001B[39mvlog(\n\u001B[0;32m    485\u001B[0m         \u001B[38;5;241m3\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to convert \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m to tensor: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (\u001B[38;5;28mtype\u001B[39m(element)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, e))\n\u001B[1;32m--> 487\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not build a `TypeSpec` for \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m with type \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m    488\u001B[0m     element,\n\u001B[0;32m    489\u001B[0m     \u001B[38;5;28mtype\u001B[39m(element)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m))\n",
      "\u001B[1;31mTypeError\u001B[0m: Could not build a `TypeSpec` for 0    [[[255, 255, 255], [255, 255, 255], [255, 255,...\n1    [[[255, 255, 255], [255, 255, 255], [255, 255,...\n2    [[[255, 255, 255], [255, 255, 255], [255, 255,...\ndtype: object with type Series",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [29], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m train_lidar_tensor \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mDataset\u001B[38;5;241m.\u001B[39mfrom_tensor_slices(train_lidar_proj_df)\n\u001B[0;32m      2\u001B[0m train_lidar_tensor \u001B[38;5;241m=\u001B[39m train_lidar_tensor\u001B[38;5;241m.\u001B[39mmap(load_and_preprocess_lidar, tf\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mAUTOTUNE)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(train_lidar_tensor\u001B[38;5;241m.\u001B[39melement_spec)\n",
      "File \u001B[1;32mD:\\university\\computer_vision\\python_interpreter\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:814\u001B[0m, in \u001B[0;36mDatasetV2.from_tensor_slices\u001B[1;34m(tensors, name)\u001B[0m\n\u001B[0;32m    736\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[0;32m    737\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfrom_tensor_slices\u001B[39m(tensors, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    738\u001B[0m   \u001B[38;5;124;03m\"\"\"Creates a `Dataset` whose elements are slices of the given tensors.\u001B[39;00m\n\u001B[0;32m    739\u001B[0m \n\u001B[0;32m    740\u001B[0m \u001B[38;5;124;03m  The given tensors are sliced along their first dimension. This operation\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    812\u001B[0m \u001B[38;5;124;03m    Dataset: A `Dataset`.\u001B[39;00m\n\u001B[0;32m    813\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[1;32m--> 814\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mTensorSliceDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\university\\computer_vision\\python_interpreter\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:4708\u001B[0m, in \u001B[0;36mTensorSliceDataset.__init__\u001B[1;34m(self, element, is_files, name)\u001B[0m\n\u001B[0;32m   4706\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, element, is_files\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m   4707\u001B[0m   \u001B[38;5;124;03m\"\"\"See `Dataset.from_tensor_slices()` for details.\"\"\"\u001B[39;00m\n\u001B[1;32m-> 4708\u001B[0m   element \u001B[38;5;241m=\u001B[39m \u001B[43mstructure\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnormalize_element\u001B[49m\u001B[43m(\u001B[49m\u001B[43melement\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4709\u001B[0m   batched_spec \u001B[38;5;241m=\u001B[39m structure\u001B[38;5;241m.\u001B[39mtype_spec_from_value(element)\n\u001B[0;32m   4710\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tensors \u001B[38;5;241m=\u001B[39m structure\u001B[38;5;241m.\u001B[39mto_batched_tensor_list(batched_spec, element)\n",
      "File \u001B[1;32mD:\\university\\computer_vision\\python_interpreter\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py:108\u001B[0m, in \u001B[0;36mnormalize_element\u001B[1;34m(element, element_signature)\u001B[0m\n\u001B[0;32m    103\u001B[0m     spec \u001B[38;5;241m=\u001B[39m type_spec_from_value(t, use_fallback\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m    104\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m    105\u001B[0m   \u001B[38;5;66;03m# TypeError indicates it was not possible to compute a `TypeSpec` for\u001B[39;00m\n\u001B[0;32m    106\u001B[0m   \u001B[38;5;66;03m# the value. As a fallback try converting the value to a tensor.\u001B[39;00m\n\u001B[0;32m    107\u001B[0m   normalized_components\u001B[38;5;241m.\u001B[39mappend(\n\u001B[1;32m--> 108\u001B[0m       \u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_to_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcomponent_\u001B[39;49m\u001B[38;5;132;43;01m%d\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m%\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    109\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    110\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(spec, sparse_tensor\u001B[38;5;241m.\u001B[39mSparseTensorSpec):\n",
      "File \u001B[1;32mD:\\university\\computer_vision\\python_interpreter\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py:183\u001B[0m, in \u001B[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    181\u001B[0m   \u001B[38;5;28;01mwith\u001B[39;00m Trace(trace_name, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mtrace_kwargs):\n\u001B[0;32m    182\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m--> 183\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\university\\computer_vision\\python_interpreter\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1638\u001B[0m, in \u001B[0;36mconvert_to_tensor\u001B[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001B[0m\n\u001B[0;32m   1629\u001B[0m       \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m   1630\u001B[0m           _add_error_prefix(\n\u001B[0;32m   1631\u001B[0m               \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConversion function \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconversion_func\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m for type \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1634\u001B[0m               \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mactual = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mret\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mbase_dtype\u001B[38;5;241m.\u001B[39mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1635\u001B[0m               name\u001B[38;5;241m=\u001B[39mname))\n\u001B[0;32m   1637\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ret \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1638\u001B[0m   ret \u001B[38;5;241m=\u001B[39m \u001B[43mconversion_func\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mas_ref\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mas_ref\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1640\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ret \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28mNotImplemented\u001B[39m:\n\u001B[0;32m   1641\u001B[0m   \u001B[38;5;28;01mcontinue\u001B[39;00m\n",
      "File \u001B[1;32mD:\\university\\computer_vision\\python_interpreter\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:343\u001B[0m, in \u001B[0;36m_constant_tensor_conversion_function\u001B[1;34m(v, dtype, name, as_ref)\u001B[0m\n\u001B[0;32m    340\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_constant_tensor_conversion_function\u001B[39m(v, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    341\u001B[0m                                          as_ref\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m    342\u001B[0m   _ \u001B[38;5;241m=\u001B[39m as_ref\n\u001B[1;32m--> 343\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mconstant\u001B[49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\university\\computer_vision\\python_interpreter\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:267\u001B[0m, in \u001B[0;36mconstant\u001B[1;34m(value, dtype, shape, name)\u001B[0m\n\u001B[0;32m    170\u001B[0m \u001B[38;5;129m@tf_export\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconstant\u001B[39m\u001B[38;5;124m\"\u001B[39m, v1\u001B[38;5;241m=\u001B[39m[])\n\u001B[0;32m    171\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconstant\u001B[39m(value, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, shape\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConst\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    172\u001B[0m   \u001B[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001B[39;00m\n\u001B[0;32m    173\u001B[0m \n\u001B[0;32m    174\u001B[0m \u001B[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    265\u001B[0m \u001B[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001B[39;00m\n\u001B[0;32m    266\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[1;32m--> 267\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_constant_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverify_shape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    268\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mallow_broadcast\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\university\\computer_vision\\python_interpreter\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:279\u001B[0m, in \u001B[0;36m_constant_impl\u001B[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001B[0m\n\u001B[0;32m    277\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m trace\u001B[38;5;241m.\u001B[39mTrace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf.constant\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    278\u001B[0m       \u001B[38;5;28;01mreturn\u001B[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001B[1;32m--> 279\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_constant_eager_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverify_shape\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    281\u001B[0m g \u001B[38;5;241m=\u001B[39m ops\u001B[38;5;241m.\u001B[39mget_default_graph()\n\u001B[0;32m    282\u001B[0m tensor_value \u001B[38;5;241m=\u001B[39m attr_value_pb2\u001B[38;5;241m.\u001B[39mAttrValue()\n",
      "File \u001B[1;32mD:\\university\\computer_vision\\python_interpreter\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:304\u001B[0m, in \u001B[0;36m_constant_eager_impl\u001B[1;34m(ctx, value, dtype, shape, verify_shape)\u001B[0m\n\u001B[0;32m    302\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_constant_eager_impl\u001B[39m(ctx, value, dtype, shape, verify_shape):\n\u001B[0;32m    303\u001B[0m   \u001B[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 304\u001B[0m   t \u001B[38;5;241m=\u001B[39m \u001B[43mconvert_to_eager_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    305\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m shape \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    306\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\n",
      "File \u001B[1;32mD:\\university\\computer_vision\\python_interpreter\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001B[0m, in \u001B[0;36mconvert_to_eager_tensor\u001B[1;34m(value, ctx, dtype)\u001B[0m\n\u001B[0;32m    100\u001B[0m     dtype \u001B[38;5;241m=\u001B[39m dtypes\u001B[38;5;241m.\u001B[39mas_dtype(dtype)\u001B[38;5;241m.\u001B[39mas_datatype_enum\n\u001B[0;32m    101\u001B[0m ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m--> 102\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEagerTensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mValueError\u001B[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray)."
     ]
    }
   ],
   "source": [
    "train_lidar_tensor = tf.data.Dataset.from_tensor_slices(train_lidar_proj_df)\n",
    "train_lidar_tensor = train_lidar_tensor.map(load_and_preprocess_lidar, tf.data.experimental.AUTOTUNE)\n",
    "print(train_lidar_tensor.element_spec)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualize the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Visualize image data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def view_image_input(image_dir, labels_dir, input_file_name):\n",
    "    img_path = image_dir + \"/\" + input_file_name\n",
    "    img = mpimg.imread(img_path)\n",
    "\n",
    "    segm_path = labels_dir + \"/\" + input_file_name\n",
    "    segm = mpimg.imread(segm_path)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 15))\n",
    "\n",
    "    ax1.set_title(\"Input image\")\n",
    "    ax1.imshow(img)\n",
    "\n",
    "    ax2.set_title(\"Segmentation mask\")\n",
    "    ax2.imshow(segm)\n",
    "\n",
    "    plt.show(block=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Real image sample:\")\n",
    "view_image_input(TRAIN_BASE_DIR + \"/file_1\" + IMAGE_DIR, TRAIN_BASE_DIR + \"/file_1\" + LABELS_DIR, \"0000000080.png\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Visualize Lidar data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def view_lidar_plot(base_dir, input_file_name, calibration):\n",
    "    lidar_path = os.path.join(base_dir, input_file_name)\n",
    "    pcd = np.load(lidar_path)\n",
    "    intensities = pcd[:, 3]\n",
    "\n",
    "    _ = plt.figure(figsize=(15, 10))\n",
    "    ax = plt.axes(projection='3d')\n",
    "    ax.set_title(\"Lidar point cloud\")\n",
    "    x = pcd[:, 0]\n",
    "    y = pcd[:, 1]\n",
    "    z = pcd[:, 2]\n",
    "    ax.scatter3D(x, y, z, c=intensities, linewidth=0.3)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def view_lidar_3d(base_dir, input_file_name):\n",
    "    lidar_path = os.path.join(base_dir, input_file_name).replace('\\\\', '/')\n",
    "    pcd = np.load(lidar_path)\n",
    "    pcd = pcd[:, :3]\n",
    "    point_cloud = o3d.geometry.PointCloud()\n",
    "    point_cloud.points = o3d.utility.Vector3dVector(pcd)\n",
    "    o3d.visualization.draw_geometries([point_cloud])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# real road scenario\n",
    "view_lidar_3d(TRAIN_BASE_DIR + \"/file_1\" + VELODYNE_DIR, \"0000000080.npy\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lost and Found dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lost_found_dataset, info = tfds.load('lost_and_found',\n",
    "                                     with_info=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The lost and found dataset contains 2104 annotated frames gathered from 112 video seqeunces: http://wwwlehre.dhbw-stuttgart.de/~sgehrig/lostAndFoundDataset/index.html"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "info.features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lost_found_dataset['train']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "View a few sample images from the dataset:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_ = tfds.visualization.show_examples(lost_found_dataset['train'], info, image_key=\"image_left\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_ = tfds.visualization.show_examples(lost_found_dataset['train'], info, image_key=\"segmentation_label\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lost_found_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
